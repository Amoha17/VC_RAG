# ğŸ™ï¸ VC_RAG: Multilingual Voice-Based Retrieval-Augmented Generation System

This is a prototype voice assistant for visually impaired users. It uses a multilingual speech-to-text â†’ semantic search â†’ LLM â†’ text-to-speech pipeline.

---

## ğŸ”§ Features

- ğŸ¤ Speech-to-Text using OpenAI Whisper
- ğŸ” Semantic Search using LangChain + ChromaDB
- ğŸ§  LLM-based question answering using LlamaCpp
- ğŸ—£ï¸ Text-to-Speech using Google TTS (gTTS) + pygame
- ğŸŒ Multilingual support (English, Hindi, Telugu, etc.)

---

## ğŸ“ Project Structure

VC_RAG/
â”œâ”€â”€ data/
â”‚ â””â”€â”€ documents/ # Source PDFs for knowledge base
|
â”œâ”€â”€ src/
â”‚ â”œâ”€â”€ stt_whisper.py # Whisper-based STT
â”‚ â”œâ”€â”€ semantic_search.py # RAG logic with LangChain
â”‚ â”œâ”€â”€ rag_wrapper.py # Wrapper to use the RAG system
â”‚ â”œâ”€â”€ tts_engine.py # gTTS-based TTS
â”‚ â”œâ”€â”€ vc_pipeline.py # End-to-end voice pipeline
â”‚ â””â”€â”€ test.py # Pipeline test script
â”œâ”€â”€ models/
â”‚ â””â”€â”€ ggml-model-q4_0.gguf # LLaMA model file (not included)
â”œâ”€â”€ requirements.txt # Python dependencies
â””â”€â”€ README.md


---

## ğŸš€ Setup

### 1. ğŸ Create Virtual Environment

```bash
python -m venv .venv
source .venv/bin/activate  # On Windows: .venv\Scripts\activate


2.  Download & Place Models
LLaMA GGUF model â†’ Place it in models/

Whisper â†’ Automatically downloaded by openai-whisper library


Run the Full Pipeline:
python src/test.py

This will:

Transcribe a voice file (WAV)
Search documents semantically
Answer using LLM
Read out the answer

Notes
Use .wav files (16kHz, mono) for input audio.
For multilingual support, language is auto-detected using langdetect.
TTS output is played using pygame and then cleaned up.

Credits

OpenAI Whisper
LangChain
gTTS
LlamaCpp
ChromaDB

Future Work

Mic input
REST API (Flask/FastAPI)
Better TTS voices (e.g., Coqui TTS)
UI for visually impaired users
